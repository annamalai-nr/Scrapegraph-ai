{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add these imports\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the scraping pipeline\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"model\": \"openai/gpt-4o-mini\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"headless\": False,\n",
    "    \"temperature\": 0.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '''Provide the following information about the article: \\n\n",
    "            1. Summary of the article\\n\n",
    "            2. Narrate in detail every image in the article (images could be block diagrams, charts, etc.)\\n\n",
    "            3. Extract and provide all LLM prompts mentioned in the article.\\n\n",
    "            Return everything in JSON format.'''\n",
    "            \n",
    "p2 = '''Extract all the content from the webpage without missing a single word.  \n",
    "1. Since there may be a large amount of text, divide it into chunks. Each chunk should be semantically relevant (for example, a paragraph or a group of sentences that discuss a single topic). Each chunk must contain at least 3 sentences.\n",
    "2. For each chunk, provide the following metadata:\n",
    "    - Entities mentioned in the chunk  \n",
    "    - Topics discussed in the chunk  \n",
    "    - Sentiment of the chunk (Positive, Negative, or Neutral)  \n",
    "    - Keywords to improve the chunk's searchability  \n",
    "    - Search queries that the chunk can answer  \n",
    "    - If the chunk contains any images or tables, provide a detailed description of them  \n",
    "    - A contextual summary that situates this chunk within the overall document/webpage for the purposes of enhancing search retrieval.\n",
    "\n",
    "3. Return everything in JSON format according to the following schema:\n",
    "\n",
    "    {\n",
    "      \"chunks\": [\n",
    "        {\n",
    "          \"text\": \"...\",\n",
    "          \"metadata\": {\n",
    "            \"entities\": [\"...\", \"...\"],\n",
    "            \"topics\": [\"...\", \"...\"],\n",
    "            \"sentiment\": \"...\",\n",
    "            \"keywords\": [\"...\", \"...\"],\n",
    "            \"search_queries\": [\"...\", \"...\"],\n",
    "            \"image_narration\": \"...\",\n",
    "            \"table_narration\": \"...\",\n",
    "            \"summary\": \"...\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "4. Only return the JSON. Do not include any other text, comments, or symbols (such as backticks).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing Fetch Node ---\n",
      "--- (Fetching HTML from: https://www.anthropic.com/news/contextual-retrieval) ---\n",
      "--- Executing ParseNode Node ---\n",
      "--- Executing GenerateAnswer Node ---\n"
     ]
    }
   ],
   "source": [
    "# Create the SmartScraperGraph instance\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "    prompt=p2,\n",
    "    source=\"https://www.anthropic.com/news/contextual-retrieval\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "result = smart_scraper_graph.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks [{'text': \"For an AI model to be useful in specific contexts, it often needs access to background knowledge. For example, customer support chatbots need knowledge about the specific business they're being used for, and legal analyst bots need to know about a vast array of past cases. Developers typically enhance an AI model's knowledge using Retrieval-Augmented Generation (RAG).\", 'metadata': {'entities': ['AI model', 'customer support chatbots', 'legal analyst bots', 'Retrieval-Augmented Generation'], 'topics': ['AI models', 'contextual knowledge', 'RAG'], 'sentiment': 'Neutral', 'keywords': ['AI', 'contextual knowledge', 'RAG', 'customer support', 'legal analysis'], 'search_queries': ['What is RAG?', 'How do AI models use background knowledge?', 'Importance of contextual knowledge in AI'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk introduces the necessity of background knowledge for AI models, particularly in customer support and legal contexts, and mentions the method of Retrieval-Augmented Generation (RAG) as a solution.'}}, {'text': \"The problem is that traditional RAG solutions remove context when encoding information, which often results in the system failing to retrieve the relevant information from the knowledge base. In this post, we outline a method that dramatically improves the retrieval step in RAG. The method is called 'Contextual Retrieval' and uses two sub-techniques: Contextual Embeddings and Contextual BM25.\", 'metadata': {'entities': ['traditional RAG', 'Contextual Retrieval', 'Contextual Embeddings', 'Contextual BM25'], 'topics': ['RAG solutions', 'Contextual Retrieval', 'information retrieval'], 'sentiment': 'Neutral', 'keywords': ['traditional RAG', 'Contextual Retrieval', 'information retrieval techniques'], 'search_queries': ['What is Contextual Retrieval?', 'How does Contextual Retrieval improve RAG?', 'What are Contextual Embeddings?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the limitations of traditional RAG solutions in retaining context and introduces the new method of Contextual Retrieval, which aims to enhance retrieval accuracy.'}}, {'text': 'This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks. You can easily deploy your own Contextual Retrieval solution with Claude with our cookbook.', 'metadata': {'entities': ['Contextual Retrieval', 'Claude'], 'topics': ['retrieval accuracy', 'performance improvement', 'deployment'], 'sentiment': 'Positive', 'keywords': ['failed retrievals', 'retrieval accuracy', 'Contextual Retrieval solution'], 'search_queries': ['How to deploy Contextual Retrieval?', 'What are the benefits of Contextual Retrieval?', 'How does reranking improve retrieval?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk highlights the effectiveness of Contextual Retrieval in improving retrieval accuracy and mentions the availability of a cookbook for deployment.'}}, {'text': 'Sometimes the simplest solution is the best. If your knowledge base is smaller than 200,000 tokens (about 500 pages of material), you can just include the entire knowledge base in the prompt that you give the model, with no need for RAG or similar methods. A few weeks ago, we released prompt caching for Claude, which makes this approach significantly faster and more cost-effective.', 'metadata': {'entities': ['knowledge base', 'Claude', 'prompt caching'], 'topics': ['knowledge base management', 'prompt caching', 'cost-effectiveness'], 'sentiment': 'Positive', 'keywords': ['knowledge base', 'prompt caching', 'cost-effective solutions'], 'search_queries': ['What is prompt caching?', 'How to manage a small knowledge base?', 'Benefits of using prompt caching with Claude'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the advantages of using prompt caching for smaller knowledge bases and emphasizes the simplicity of including the entire knowledge base in the model prompt.'}}, {'text': \"For larger knowledge bases that don't fit within the context window, RAG is the typical solution. RAG works by preprocessing a knowledge base using the following steps: 1. Break down the knowledge base (the 'corpus' of documents) into smaller chunks of text, usually no more than a few hundred tokens; 2. Use an embedding model to convert these chunks into vector embeddings that encode meaning; 3. Store these embeddings in a vector database that allows for searching by semantic similarity.\", 'metadata': {'entities': ['RAG', 'knowledge base', 'embedding model', 'vector database'], 'topics': ['knowledge base processing', 'RAG methodology', 'semantic similarity'], 'sentiment': 'Neutral', 'keywords': ['RAG', 'knowledge base', 'embedding models', 'vector embeddings'], 'search_queries': ['How does RAG work?', 'What are the steps in RAG?', 'What is a vector database?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk outlines the typical RAG process for handling larger knowledge bases, detailing the steps involved in preprocessing and storing embeddings.'}}, {'text': 'At runtime, when a user inputs a query to the model, the vector database is used to find the most relevant chunks based on semantic similarity to the query. Then, the most relevant chunks are added to the prompt sent to the generative model. While embedding models excel at capturing semantic relationships, they can miss crucial exact matches.', 'metadata': {'entities': ['user query', 'vector database', 'generative model'], 'topics': ['query processing', 'semantic similarity', 'embedding models'], 'sentiment': 'Neutral', 'keywords': ['user query', 'vector database', 'semantic relationships'], 'search_queries': ['How does a vector database work?', 'What is semantic similarity?', 'How are queries processed in RAG?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk explains how the vector database retrieves relevant chunks based on user queries and highlights the strengths and weaknesses of embedding models.'}}, {'text': \"Fortunately, there’s an older technique that can assist in these situations. BM25 (Best Matching 25) is a ranking function that uses lexical matching to find precise word or phrase matches. It's particularly effective for queries that include unique identifiers or technical terms.\", 'metadata': {'entities': ['BM25', 'Best Matching 25'], 'topics': ['ranking functions', 'lexical matching', 'query processing'], 'sentiment': 'Neutral', 'keywords': ['BM25', 'ranking function', 'lexical matching'], 'search_queries': ['What is BM25?', 'How does BM25 work?', 'When to use BM25?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk introduces BM25 as a complementary technique to embedding models, emphasizing its effectiveness in finding exact matches for specific queries.'}}, {'text': 'BM25 works by building upon the TF-IDF (Term Frequency-Inverse Document Frequency) concept. TF-IDF measures how important a word is to a document in a collection. BM25 refines this by considering document length and applying a saturation function to term frequency, which helps prevent common words from dominating the results.', 'metadata': {'entities': ['BM25', 'TF-IDF'], 'topics': ['information retrieval', 'term frequency', 'document importance'], 'sentiment': 'Neutral', 'keywords': ['BM25', 'TF-IDF', 'document length', 'term frequency'], 'search_queries': ['How does BM25 improve upon TF-IDF?', 'What is TF-IDF?', 'How is document importance measured?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk elaborates on the workings of BM25 and its relationship with the TF-IDF concept, explaining how it enhances document retrieval.'}}, {'text': \"Here’s how BM25 can succeed where semantic embeddings fail: Suppose a user queries 'Error code TS-999' in a technical support database. An embedding model might find content about error codes in general, but could miss the exact 'TS-999' match. BM25 looks for this specific text string to identify the relevant documentation.\", 'metadata': {'entities': ['BM25', 'Error code TS-999'], 'topics': ['technical support', 'error codes', 'information retrieval'], 'sentiment': 'Neutral', 'keywords': ['BM25', 'error code', 'technical support'], 'search_queries': ['How does BM25 handle specific queries?', 'What is the advantage of BM25 for technical support?', 'Can BM25 find exact matches?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk provides a practical example of how BM25 can effectively retrieve specific information that embedding models might overlook.'}}, {'text': \"RAG solutions can more accurately retrieve the most applicable chunks by combining the embeddings and BM25 techniques using the following steps: 1. Break down the knowledge base (the 'corpus' of documents) into smaller chunks of text, usually no more than a few hundred tokens; 2. Create TF-IDF encodings and semantic embeddings for these chunks; 3. Use BM25 to find top chunks based on exact matches; 4. Use embeddings to find top chunks based on semantic similarity; 5. Combine and deduplicate results from (3) and (4) using rank fusion techniques; 6. Add the top-K chunks to the prompt to generate the response.\", 'metadata': {'entities': ['RAG', 'BM25', 'TF-IDF'], 'topics': ['RAG methodology', 'chunk processing', 'information retrieval'], 'sentiment': 'Neutral', 'keywords': ['RAG', 'BM25', 'TF-IDF', 'chunk processing'], 'search_queries': ['How to combine BM25 and embeddings?', 'What are the steps in RAG?', 'How to improve retrieval accuracy?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk outlines a detailed process for combining BM25 and embedding techniques within RAG to enhance retrieval accuracy.'}}, {'text': 'By leveraging both BM25 and embedding models, traditional RAG systems can provide more comprehensive and accurate results, balancing precise term matching with broader semantic understanding. A Standard Retrieval-Augmented Generation (RAG) system that uses both embeddings and Best Match 25 (BM25) to retrieve information. TF-IDF (term frequency-inverse document frequency) measures word importance and forms the basis for BM25.', 'metadata': {'entities': ['BM25', 'RAG', 'TF-IDF'], 'topics': ['information retrieval', 'RAG systems', 'semantic understanding'], 'sentiment': 'Positive', 'keywords': ['BM25', 'RAG', 'TF-IDF', 'information retrieval'], 'search_queries': ['What is a standard RAG system?', 'How do BM25 and embeddings work together?', 'What is the role of TF-IDF in RAG?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk emphasizes the advantages of combining BM25 with embedding models in RAG systems for improved retrieval results.'}}, {'text': 'This approach allows you to cost-effectively scale to enormous knowledge bases, far beyond what could fit in a single prompt. But these traditional RAG systems have a significant limitation: they often destroy context.', 'metadata': {'entities': ['RAG systems'], 'topics': ['scalability', 'context preservation'], 'sentiment': 'Neutral', 'keywords': ['scalability', 'RAG', 'context'], 'search_queries': ['What are the limitations of traditional RAG?', 'How to scale knowledge bases effectively?', 'Why is context important in RAG?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the scalability of traditional RAG systems while highlighting their limitation in preserving context.'}}, {'text': \"In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context. For example, imagine you had a collection of financial information (say, U.S. SEC filings) embedded in your knowledge base, and you received the following question: 'What was the revenue growth for ACME Corp in Q2 2023?'\", 'metadata': {'entities': ['RAG', 'U.S. SEC filings', 'ACME Corp'], 'topics': ['document chunking', 'contextual issues', 'financial information'], 'sentiment': 'Neutral', 'keywords': ['RAG', 'document chunking', 'financial information'], 'search_queries': ['What are the issues with document chunking?', 'How does context affect retrieval?', 'What is the revenue growth for ACME Corp?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk illustrates the potential issues with traditional RAG when context is lost, using a financial example to highlight the problem.'}}, {'text': \"A relevant chunk might contain the text: 'The company's revenue grew by 3% over the previous quarter.' However, this chunk on its own doesn’t specify which company it’s referring to or the relevant time period, making it difficult to retrieve the right information or use the information effectively.\", 'metadata': {'entities': ['ACME Corp'], 'topics': ['contextual information', 'retrieval challenges'], 'sentiment': 'Neutral', 'keywords': ['revenue growth', 'contextual information', 'retrieval challenges'], 'search_queries': ['How to improve retrieval accuracy?', 'What information is needed for effective retrieval?', \"What was ACME Corp's revenue growth?\"], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk emphasizes the importance of context in retrieval, illustrating how a lack of context can hinder effective information retrieval.'}}, {'text': \"Contextual Retrieval solves this problem by prepending chunk-specific explanatory context to each chunk before embedding ('Contextual Embeddings') and creating the BM25 index ('Contextual BM25'). Let’s return to our SEC filings collection example. Here’s an example of how a chunk might be transformed: original_chunk = 'The company's revenue grew by 3% over the previous quarter.' contextualized_chunk = 'This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.'\", 'metadata': {'entities': ['Contextual Retrieval', 'ACME Corp', 'SEC filings'], 'topics': ['contextual embedding', 'information retrieval'], 'sentiment': 'Positive', 'keywords': ['Contextual Retrieval', 'contextual embeddings', 'information retrieval'], 'search_queries': ['What is Contextual Retrieval?', 'How does Contextual Retrieval work?', 'How to contextualize chunks?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk introduces Contextual Retrieval as a solution to the context problem in traditional RAG, providing an example of how chunks can be contextualized.'}}, {'text': 'It is worth noting that other approaches to using context to improve retrieval have been proposed in the past. Other proposals include: adding generic document summaries to chunks (we experimented and saw very limited gains), hypothetical document embedding, and summary-based indexing (we evaluated and saw low performance). These methods differ from what is proposed in this post.', 'metadata': {'entities': ['document summaries', 'hypothetical document embedding', 'summary-based indexing'], 'topics': ['contextual retrieval methods', 'information retrieval'], 'sentiment': 'Neutral', 'keywords': ['contextual retrieval', 'document summaries', 'summary-based indexing'], 'search_queries': ['What are alternative methods to Contextual Retrieval?', 'How effective are document summaries?', 'What is summary-based indexing?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses alternative methods to improve retrieval context and highlights their limitations compared to Contextual Retrieval.'}}, {'text': 'Of course, it would be far too much work to manually annotate the thousands or even millions of chunks in a knowledge base. To implement Contextual Retrieval, we turn to Claude. We’ve written a prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document.', 'metadata': {'entities': ['Claude'], 'topics': ['Contextual Retrieval implementation', 'chunk annotation'], 'sentiment': 'Neutral', 'keywords': ['Contextual Retrieval', 'chunk-specific context', 'Claude'], 'search_queries': ['How to implement Contextual Retrieval?', 'What is the role of Claude in Contextual Retrieval?', 'How to annotate chunks?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk explains the implementation of Contextual Retrieval using Claude, emphasizing the need for automated chunk-specific context generation.'}}, {'text': 'We used the following Claude 3 Haiku prompt to generate context for each chunk: {{WHOLE_DOCUMENT}} Here is the chunk we want to situate within the whole document {{CHUNK_CONTENT}} Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.', 'metadata': {'entities': ['Claude 3 Haiku'], 'topics': ['prompt design', 'context generation'], 'sentiment': 'Neutral', 'keywords': ['Claude 3 Haiku', 'prompt design', 'context generation'], 'search_queries': ['What is the Claude 3 Haiku prompt?', 'How to design prompts for context generation?', 'How to improve search retrieval?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk provides details on the specific prompt used in Claude to generate context for chunks, highlighting its role in improving search retrieval.'}}, {'text': 'The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index. Here’s what the preprocessing flow looks like in practice: Contextual Retrieval is a preprocessing technique that improves retrieval accuracy.', 'metadata': {'entities': ['Contextual Retrieval'], 'topics': ['preprocessing techniques', 'retrieval accuracy'], 'sentiment': 'Positive', 'keywords': ['preprocessing', 'Contextual Retrieval', 'retrieval accuracy'], 'search_queries': ['What is the preprocessing flow for Contextual Retrieval?', 'How does Contextual Retrieval improve accuracy?', 'What is the role of contextual text?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk describes the preprocessing flow of Contextual Retrieval and its impact on improving retrieval accuracy.'}}, {'text': 'If you’re interested in using Contextual Retrieval, you can get started with our cookbook. Contextual Retrieval is uniquely possible at low cost with Claude, thanks to the special prompt caching feature we mentioned above. With prompt caching, you don’t need to pass in the reference document for every chunk.', 'metadata': {'entities': ['Contextual Retrieval', 'Claude', 'prompt caching'], 'topics': ['cost-effectiveness', 'Contextual Retrieval implementation'], 'sentiment': 'Positive', 'keywords': ['Contextual Retrieval', 'prompt caching', 'cost-effective solutions'], 'search_queries': ['How to use Contextual Retrieval?', 'What is the cost of using Contextual Retrieval?', 'How does prompt caching work?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk highlights the cost-effectiveness of using Contextual Retrieval with Claude and encourages users to refer to the cookbook for implementation.'}}, {'text': 'You simply load the document into the cache once and then reference the previously cached content. Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, the one-time cost to generate contextualized chunks is $1.02 per million document tokens.', 'metadata': {'entities': ['document cache'], 'topics': ['cost analysis', 'Contextual Retrieval'], 'sentiment': 'Neutral', 'keywords': ['document cache', 'cost analysis', 'contextualized chunks'], 'search_queries': ['What is the cost of generating contextualized chunks?', 'How does document caching work?', 'What are the assumptions for cost analysis?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk provides a cost analysis for generating contextualized chunks using document caching, detailing the assumptions made in the calculation.'}}, {'text': 'Methodology We experimented across various knowledge domains (codebases, fiction, ArXiv papers, Science Papers), embedding models, retrieval strategies, and evaluation metrics. We’ve included a few examples of the questions and answers we used for each domain in Appendix II.', 'metadata': {'entities': ['codebases', 'fiction', 'ArXiv papers', 'Science Papers'], 'topics': ['methodology', 'experimentation', 'evaluation metrics'], 'sentiment': 'Neutral', 'keywords': ['methodology', 'experimentation', 'evaluation metrics'], 'search_queries': ['What methodologies were used in the experiments?', 'What domains were tested?', 'How were evaluation metrics defined?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk outlines the methodology used in the experiments across various knowledge domains and mentions the inclusion of examples in the appendix.'}}, {'text': 'The graphs below show the average performance across all knowledge domains with the top-performing embedding configuration (Gemini Text 004) and retrieving the top-20-chunks. We use 1 minus recall@20 as our evaluation metric, which measures the percentage of relevant documents that fail to be retrieved within the top 20 chunks.', 'metadata': {'entities': ['Gemini Text 004'], 'topics': ['performance evaluation', 'embedding configuration'], 'sentiment': 'Neutral', 'keywords': ['performance evaluation', 'recall@20', 'embedding configuration'], 'search_queries': ['What is recall@20?', 'How was performance evaluated?', 'What is the significance of Gemini Text 004?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the evaluation metrics used to assess performance across knowledge domains, specifically focusing on recall@20.'}}, {'text': 'You can see the full results in the appendix - contextualizing improves performance in every embedding-source combination we evaluated. Performance improvements Our experiments showed that: * Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35% (5.7% → 3.7%). * Combining Contextual Embeddings and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 49% (5.7% → 2.9%).', 'metadata': {'entities': ['Contextual Embeddings', 'Contextual BM25'], 'topics': ['performance improvements', 'retrieval accuracy'], 'sentiment': 'Positive', 'keywords': ['performance improvements', 'retrieval failure rate', 'Contextual Embeddings'], 'search_queries': ['What are the performance improvements from contextualizing?', 'How effective are Contextual Embeddings?', 'What is the impact of combining techniques?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk summarizes the performance improvements observed in experiments, highlighting the effectiveness of Contextual Embeddings and their combination with Contextual BM25.'}}, {'text': 'Implementation considerations When implementing Contextual Retrieval, there are a few considerations to keep in mind: 1. Chunk boundaries: Consider how you split your documents into chunks. The choice of chunk size, chunk boundary, and chunk overlap can affect retrieval performance. 2. Embedding model: Whereas Contextual Retrieval improves performance across all embedding models we tested, some models may benefit more than others.', 'metadata': {'entities': ['Contextual Retrieval', 'embedding models'], 'topics': ['implementation considerations', 'chunk processing'], 'sentiment': 'Neutral', 'keywords': ['implementation considerations', 'chunk boundaries', 'embedding models'], 'search_queries': ['What are the considerations for implementing Contextual Retrieval?', 'How do chunk boundaries affect performance?', 'Which embedding models are most effective?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk outlines key considerations for implementing Contextual Retrieval, focusing on chunk boundaries and the choice of embedding models.'}}, {'text': 'We found Gemini and Voyage embeddings to be particularly effective. 3. Custom contextualizer prompts: While the generic prompt we provided works well, you may be able to achieve even better results with prompts tailored to your specific domain or use case (for example, including a glossary of key terms that might only be defined in other documents in the knowledge base).', 'metadata': {'entities': ['Gemini', 'Voyage'], 'topics': ['embedding models', 'custom prompts'], 'sentiment': 'Positive', 'keywords': ['embedding models', 'custom prompts', 'domain-specific'], 'search_queries': ['What are the best embedding models?', 'How to create custom prompts?', 'What is the importance of domain-specific prompts?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the effectiveness of specific embedding models and the potential benefits of using custom prompts tailored to specific domains.'}}, {'text': \"Adding more chunks into the context window increases the chances that you include the relevant information. However, more information can be distracting for models so there's a limit to this. We tried delivering 5, 10, and 20 chunks, and found using 20 to be the most performant of these options (see appendix for comparisons) but it’s worth experimenting on your use case.\", 'metadata': {'entities': ['context window'], 'topics': ['information retrieval', 'chunk delivery'], 'sentiment': 'Neutral', 'keywords': ['context window', 'chunk delivery', 'performance'], 'search_queries': ['How many chunks should be delivered?', 'What is the impact of chunk delivery on performance?', 'How to optimize chunk delivery?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the balance between including more chunks in the context window and the potential distractions they may cause, emphasizing the importance of experimentation.'}}, {'text': 'Always run evals: Response generation may be improved by passing it the contextualized chunk and distinguishing between what is context and what is the chunk. Further boosting performance with Reranking In a final step, we can combine Contextual Retrieval with another technique to give even more performance improvements.', 'metadata': {'entities': ['Reranking', 'Contextual Retrieval'], 'topics': ['performance improvement', 'response generation'], 'sentiment': 'Positive', 'keywords': ['Reranking', 'performance improvement', 'response generation'], 'search_queries': ['How to improve response generation?', 'What is Reranking?', 'How does Reranking enhance performance?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk emphasizes the importance of evaluation in response generation and introduces Reranking as a technique to further enhance performance.'}}, {'text': 'In traditional RAG, the AI system searches its knowledge base to find the potentially relevant information chunks. With large knowledge bases, this initial retrieval often returns a lot of chunks—sometimes hundreds—of varying relevance and importance. Reranking is a commonly used filtering technique to ensure that only the most relevant chunks are passed to the model.', 'metadata': {'entities': ['RAG', 'Reranking'], 'topics': ['information retrieval', 'filtering techniques'], 'sentiment': 'Neutral', 'keywords': ['RAG', 'Reranking', 'information retrieval'], 'search_queries': ['What is the role of Reranking in RAG?', 'How does Reranking improve retrieval?', 'What are filtering techniques?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk explains the role of Reranking in traditional RAG systems, highlighting its importance in filtering relevant information from large knowledge bases.'}}, {'text': \"Reranking provides better responses and reduces cost and latency because the model is processing less information. The key steps are: 1. Perform initial retrieval to get the top potentially relevant chunks (we used the top 150); 2. Pass the top-N chunks, along with the user's query, through the reranking model; 3. Using a reranking model, give each chunk a score based on its relevance and importance to the prompt, then select the top-K chunks (we used the top 20); 4. Pass the top-K chunks into the model as context to generate the final result.\", 'metadata': {'entities': ['Reranking', 'user query'], 'topics': ['retrieval process', 'response generation'], 'sentiment': 'Positive', 'keywords': ['Reranking', 'retrieval process', 'response generation'], 'search_queries': ['What are the steps in Reranking?', 'How does Reranking improve responses?', 'What is the process of Reranking?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk outlines the key steps involved in the Reranking process, emphasizing its benefits in improving response quality and reducing costs.'}}, {'text': 'Combine Contextual Retrieval and Reranking to maximize retrieval accuracy. Performance improvements There are several reranking models on the market. We ran our tests with the Cohere reranker, Voyage also offers a reranker, though we did not have time to test it.', 'metadata': {'entities': ['Cohere', 'Voyage'], 'topics': ['retrieval accuracy', 'reranking models'], 'sentiment': 'Positive', 'keywords': ['Contextual Retrieval', 'Reranking', 'retrieval accuracy'], 'search_queries': ['What are the best reranking models?', 'How to combine Contextual Retrieval and Reranking?', 'What is the impact of Reranking on retrieval accuracy?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the combination of Contextual Retrieval and Reranking to enhance retrieval accuracy and mentions the reranking models tested.'}}, {'text': 'Our experiments showed that, across various domains, adding a reranking step further optimizes retrieval. Specifically, we found that Reranked Contextual Embedding and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 67% (5.7% → 1.9%).', 'metadata': {'entities': ['Reranked Contextual Embedding', 'Contextual BM25'], 'topics': ['performance optimization', 'retrieval accuracy'], 'sentiment': 'Positive', 'keywords': ['Reranked Contextual Embedding', 'Contextual BM25', 'retrieval failure rate'], 'search_queries': ['What are the results of adding Reranking?', 'How effective is Reranked Contextual Embedding?', 'What is the impact of Reranking on retrieval failure rate?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk summarizes the performance improvements observed from adding a Reranking step, highlighting its effectiveness in reducing retrieval failure rates.'}}, {'text': 'One important consideration with reranking is the impact on latency and cost, especially when reranking a large number of chunks. Because reranking adds an extra step at runtime, it inevitably adds a small amount of latency, even though the reranker scores all the chunks in parallel. There is an inherent trade-off between reranking more chunks for better performance vs. reranking fewer for lower latency and cost.', 'metadata': {'entities': ['Reranking'], 'topics': ['latency', 'cost considerations'], 'sentiment': 'Neutral', 'keywords': ['Reranking', 'latency', 'cost considerations'], 'search_queries': ['What are the trade-offs of Reranking?', 'How does Reranking affect latency?', 'What is the cost of Reranking?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk discusses the trade-offs involved in Reranking, particularly focusing on the balance between performance improvements and the associated costs and latency.'}}, {'text': 'We ran a large number of tests, comparing different combinations of all the techniques described above (embedding model, use of BM25, use of contextual retrieval, use of a reranker, and total # of top-K results retrieved), all across a variety of different dataset types. Here’s a summary of what we found: 1. Embeddings+BM25 is better than embeddings on their own; 2. Voyage and Gemini have the best embeddings of the ones we tested; 3. Passing the top-20 chunks to the model is more effective than just the top-10 or top-5; 4. Adding context to chunks improves retrieval accuracy a lot; 5. Reranking is better than no reranking; 6. All these benefits stack: to maximize performance improvements, we can combine contextual embeddings (from Voyage or Gemini) with contextual BM25, plus a reranking step, and adding the 20 chunks to the prompt.', 'metadata': {'entities': ['Embeddings', 'BM25', 'Voyage', 'Gemini'], 'topics': ['performance evaluation', 'technique comparison'], 'sentiment': 'Positive', 'keywords': ['Embeddings', 'BM25', 'performance evaluation', 'contextual retrieval'], 'search_queries': ['What are the findings from the tests?', 'How do different techniques compare?', 'What is the best approach for retrieval?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk summarizes the findings from extensive testing of various techniques for retrieval, highlighting the effectiveness of combining methods for optimal performance.'}}, {'text': 'We encourage all developers working with knowledge bases to use our cookbook to experiment with these approaches to unlock new levels of performance. Appendix I Below is a breakdown of results across datasets, embedding providers, use of BM25 in addition to embeddings, use of contextual retrieval, and use of reranking for Retrievals @ 20.', 'metadata': {'entities': ['developers', 'knowledge bases'], 'topics': ['performance improvement', 'experimenting with techniques'], 'sentiment': 'Positive', 'keywords': ['developers', 'knowledge bases', 'performance improvement'], 'search_queries': ['How to experiment with retrieval techniques?', 'What is included in the cookbook?', 'What are the results from the experiments?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk encourages developers to utilize the cookbook for experimenting with retrieval techniques and introduces the appendix for further results.'}}, {'text': 'You can see the full results in the appendix - contextualizing improves performance in every embedding-source combination we evaluated. 1 minus recall @ 20 results across data sets and embedding providers.', 'metadata': {'entities': ['recall @ 20'], 'topics': ['performance evaluation', 'results analysis'], 'sentiment': 'Positive', 'keywords': ['recall @ 20', 'performance evaluation', 'results'], 'search_queries': ['What are the results of recall @ 20?', 'How does contextualizing affect performance?', 'What is the significance of the results?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk highlights the positive impact of contextualizing on performance, referencing the results found in the appendix.'}}, {'text': 'For additional reading on chunking strategies, check out this link and this link.', 'metadata': {'entities': ['chunking strategies'], 'topics': ['additional resources'], 'sentiment': 'Neutral', 'keywords': ['chunking strategies', 'additional reading'], 'search_queries': ['What are chunking strategies?', 'Where to find more information on chunking?'], 'image_narration': 'NA', 'table_narration': 'NA', 'summary': 'This chunk provides links for further reading on chunking strategies, serving as a resource for those interested in the topic.'}}]\n"
     ]
    }
   ],
   "source": [
    "for k, v in result.items():\n",
    "    print(k, v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
